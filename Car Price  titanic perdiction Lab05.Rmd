```{r}
cars<-read.csv('https://www.dropbox.com/s/rklt5wlth5qt1y8/CarPrice_Assignment.csv?dl=1',stringsAsFactors=T,header=T)
titanic<-read.csv('https://www.dropbox.com/s/5bmos41npqiumic/Titanic.csv?dl=1',stringsAsFactors=T,header=T)
```
```{r}
carPrice<-read.csv('https://www.dropbox.com/s/rklt5wlth5qt1y8/CarPrice_Assignment.csv?dl=1',stringsAsFactors=T,header=T)
```

```{r}
head(titanic)
head(cars)
```
```{r}
library(caret)
library(rattle)
```
```{r}
set.seed(1234)

#dealing with missing values 
titanic[titanic == ""] <- NA
titanic <- titanic[complete.cases(titanic),]
#removal of non useful vars 

titanic$PassengerId <- NULL
titanic$Name <- NULL
titanic$Cabin <- NULL
titanic$Ticket <- NULL
```


```{r}
#turning it into a factor 
titanic$Survived <- as.factor(titanic$Survived)
titanic$Pclass <- as.factor(titanic$Pclass)

```





```{r}
trControl1 <- trainControl(method = "oob")
trControl2 <- trainControl(method = "repeatedcv", number=10, repeats=1)
trControl3 <- trainControl(method = "boot" )
trControl4 <- trainControl(method = "LGOCV", number=20)
trControl5 <- trainControl(method = "LOOCV")
```



```{r}
set.seed(1234)
rpart.model <- train(Survived~.,
    data = titanic,
    method = "rpart",
    metric = "Accuracy",
    trControl = trControl2)

```


```{r}
print(rpart.model)
```
```{r}
confusionMatrix.train(rpart.model)
```
```{r}
fancyRpartPlot(rpart.model$finalModel)
```


```{r}
install.packages("rf")
library(rf)
```

```{r}
set.seed(1234)
rf.model <- train(Survived~.,
    data = titanic,
    method = "rf",
    metric = "Accuracy",
    trControl = trControl2,
    tuneGrid = expand.grid(mtry=c(1,2,3,4)))

```


```{r}
print(rf.model)
```
```{r}
#You can change the number of trees generated by setting ntree, 
#but you can only try one value at a time (i.e. you cannot put several 
#values in the grid, like we do for mtry). For example

set.seed(1234)
rf.model02 <- train(Survived~.,
    data = titanic,
    method = "rf",
    metric = "Accuracy",
    ntree = 1000,
    trControl = trControl2,
    tuneGrid = expand.grid(mtry=c(1,2,3,4)))
print(rf.model02)
confusionMatrix.train(rf.model02)





```



```{r}
#you can also change the max no of leaf nodes 

set.seed(1234)
rf.model03 <- train(Survived~.,
    data = titanic,
    method = "rf",
    metric = "Accuracy",
    ntree= 300,
    maxnodes = 5,
    trControl = trControl2,
    tuneGrid = expand.grid(mtry=c(1:6)))
print(rf.model03)
confusionMatrix.train(rf.model03)

```

```{r}

#we can also specify the number of samples to be drawn for each class value
#(sampsize), when looking at a classification model. For the titanic dataset, we have 2 classes, 0 and 1. 
levels(titanic$Survived)
```
```{r}
#We can state now may of each we want in each bootstrap bag. 
#For example, if we want 50 of each class we can state

set.seed(1234)
rf.model04 <- train(Survived~.,
    data = titanic,
    method = "rf",
    metric = "Accuracy",
    ntree= 500,
    maxnodes = 5,
    sampsize = c(50,50),
    trControl = trControl2,
    tuneGrid = expand.grid(mtry=c(1,2,3,4)))
 
print(rf.model04)
confusionMatrix.train(rf.model04)
```
```{r}
#For random forest, a common evaluation method is out of bag (see lecture slides). 
#This method is not available for other algorithms covered in this lab and earlier 
#we defined a train control using this method (trControl1).


set.seed(1234)
rf.oobmodel <- train(Survived~.,
    data = titanic,
    method = "rf",
    metric = "Accuracy",
    trControl = trControl1,
    tuneGrid = expand.grid(mtry=c(1,2,3,4)))
print(rf.oobmodel)

```
```{r}
varImp(rf.model)
varImp(rf.model02)
varImp(rf.model03)
 varImp(rf.model04)
 varImp(rf.oobmodel)

 
```

```{r}
#boosted trees 
#Method "xgbTree" produces boosted trees. It takes longer to produce a model, 
#as the tree components of the ensemble are generated one at a time. 
#This is because the next tree uses the results of previous trees to 
#determine which instances it should focus on solving.

my_grid <- expand.grid(nrounds = 500,
                   max_depth = 7,
                   eta = 0.1,
                   gamma = 1,
                   colsample_bytree = 1,
                   min_child_weight = 100,
                   subsample = 1)
```




```{r}
xgboost.model <- train(Survived~.,
    data = titanic,
    method = "xgbTree",
    metric = "Accuracy",
    trControl = trainControl(method="repeatedcv", number=10, repeats=1),
    tuneGrid = my_grid)


```



```{r}

print(xgb_mod)

```
```{r}
confusionMatrix(xgb_mod)
```
```{r}
#when comparing models train control object should be the same 

results <- resamples(list(CART=rpart.model, 
randomForest = rf.model, xgboost = xgboost.model))
summary(results)

```

```{r}
scales <- list(x=list(relation="free"), y=list(relation= "free"))
dotplot(results, scales=scales, conf.level = 0.95)

```
```{r}
carPrice <- carPrice[, c(10:14,17,19:26)]

```



```{r}
head(carPrice)
```
```{r}

rpart.model_NUMPRED <- train(price~.,
    data = carPrice,
    method = "rpart",
    metric = "RMSE",
    trControl = trControl2)

```
```{r}
min(rpart.model_NUMPRED$results$RMSE)/mean(carPrice$price) * 100
```



```{r}
rpart.model_NUMPRED
```


9.	Apply rf to carPrice with price as the target feature.  
State the best mtry value, and the best RMSE. Calculate the 
proportion of RMSE out of the mean value for the target feature. 
Identify the best R-square and assess how close to 1 it is.


```{r}

```




10.	Apply xgboost to carPrice with price as the target feature.  
Calculate the proportion of RMSE out of the mean value for the target feature. 
Identify the best R-square and assess how close to 1 it is.


```{r}

```



11.	Compare the performance of the 3 algorithms on the carPrice dataset.




```{r}

```



12.	If you have time tune the rf model for carPrice by changing parameters other than mtry.

```{r}

```


