```{r}
cars<-read.csv('https://www.dropbox.com/s/rklt5wlth5qt1y8/CarPrice_Assignment.csv?dl=1',stringsAsFactors=T,header=T)
titanic<-read.csv('https://www.dropbox.com/s/5bmos41npqiumic/Titanic.csv?dl=1',stringsAsFactors=T,header=T)
```
```{r}
carPrice<-read.csv('https://www.dropbox.com/s/rklt5wlth5qt1y8/CarPrice_Assignment.csv?dl=1',stringsAsFactors=T,header=T)
```

```{r}
head(titanic)
head(cars)
```
```{r}
library(caret)
library(rattle)
```
```{r}
set.seed(1234)

#dealing with missing values 
titanic[titanic == ""] <- NA
titanic <- titanic[complete.cases(titanic),]
#removal of non useful vars 

titanic$PassengerId <- NULL
titanic$Name <- NULL
titanic$Cabin <- NULL
titanic$Ticket <- NULL
```


```{r}
#turning it into a factor 
titanic$Survived <- as.factor(titanic$Survived)
titanic$Pclass <- as.factor(titanic$Pclass)

```





```{r}
trControl1 <- trainControl(method = "oob")
trControl2 <- trainControl(method = "repeatedcv", number=10, repeats=1)
trControl3 <- trainControl(method = "boot" )
trControl4 <- trainControl(method = "LGOCV", number=20)
trControl5 <- trainControl(method = "LOOCV")
```



```{r}
set.seed(1234)
rpart.model <- train(Survived~.,
    data = titanic,
    method = "rpart",
    metric = "Accuracy",
    trControl = trControl2)

```


```{r}
print(rpart.model)
```

The best accuracy was 0.7538012  with a corresponding kappa of 0.4611742 (moderate agreement). The confusion matrix shows that  23.5% of the instances corresponded to class 0 correctly predicted and 51.9% to class 1 correctly predicted. 9.3% were class 0 instances predicted incorrectly and 15.3% were class 1 instances incorrectly classified. Proportionally, the predictions of class 0 were, therefore, worse (9.3/(23.5 + 9.3), i.e. 28.35% of class 0 instances)
 For class 1, the % errors was 15.3/(51.9+15.3), i.e. 22.76%.



```{r}
confusionMatrix.train(rpart.model)
```
```{r}
fancyRpartPlot(rpart.model$finalModel)
```


```{r}
install.packages("rf")

```

```{r}
library(rf)
```


```{r}
set.seed(1234)
rf.model <- train(Survived~.,
    data = titanic,
    method = "rf",
    metric = "Accuracy",
    trControl = trControl2,
    tuneGrid = expand.grid(mtry=c(1,2,3,4)))

```


```{r}
print(rf.model)
confusionMatrix(rf.model)
```
The accuracy is a little higher than for rpart with 78.14% (compared to 75.38%) with a  kappa value showing moderate agreement (0.4735864). The distribution of errors is quite different, with proportionally more errors where class 0 instances were misclassified but less where class 1 instances were misclassified.



```{r}
#You can change the number of trees generated by setting ntree, 
#but you can only try one value at a time (i.e. you cannot put several 
#values in the grid, like we do for mtry). For example

set.seed(1234)
rf.model02 <- train(Survived~.,
    data = titanic,
    method = "rf",
    metric = "Accuracy",
    ntree = 1000,
    trControl = trControl2,
    tuneGrid = expand.grid(mtry=c(1,2,3,4)))
print(rf.model02)
confusionMatrix.train(rf.model02)

```



```{r}
#you can also change the max no of leaf nodes 

set.seed(1234)
rf.model03 <- train(Survived~.,
    data = titanic,
    method = "rf",
    metric = "Accuracy",
    ntree= 300,
    maxnodes = 5,
    trControl = trControl2,
    tuneGrid = expand.grid(mtry=c(1:6)))
print(rf.model03)
confusionMatrix.train(rf.model03)

```

```{r}

#we can also specify the number of samples to be drawn for each class value
#(sampsize), when looking at a classification model. For the titanic dataset, we have 2 classes, 0 and 1. 
levels(titanic$Survived)
```
```{r}
#We can state now may of each we want in each bootstrap bag. 
#For example, if we want 50 of each class we can state

set.seed(1234)
rf.model04 <- train(Survived~.,
    data = titanic,
    method = "rf",
    metric = "Accuracy",
    ntree= 500,
    maxnodes = 5,
    sampsize = c(50,50),
    trControl = trControl2,
    tuneGrid = expand.grid(mtry=c(1,2,3,4)))
 
print(rf.model04)
confusionMatrix.train(rf.model04)
```
```{r}
#For random forest, a common evaluation method is out of bag (see lecture slides). 
#This method is not available for other algorithms covered in this lab and earlier 
#we defined a train control using this method (trControl1).


set.seed(1234)
rf.oobmodel <- train(Survived~.,
    data = titanic,
    method = "rf",
    metric = "Accuracy",
    trControl = trControl1,
    tuneGrid = expand.grid(mtry=c(1,2,3,4)))
print(rf.oobmodel)

```
```{r}
varImp(rf.model)
varImp(rf.model02)
varImp(rf.model03)
 varImp(rf.model04)
 varImp(rf.oobmodel)

 
```

```{r}
#boosted trees 
#Method "xgbTree" produces boosted trees. It takes longer to produce a model, 
#as the tree components of the ensemble are generated one at a time. 
#This is because the next tree uses the results of previous trees to 
#determine which instances it should focus on solving.

my_grid <- expand.grid(nrounds = 500,
                   max_depth = 7,
                   eta = 0.1,
                   gamma = 1,
                   colsample_bytree = 1,
                   min_child_weight = 100,
                   subsample = 1)
```




```{r}
xgboost.model <- train(Survived~.,
    data = titanic,
    method = "xgbTree",
    metric = "Accuracy",
    trControl = trainControl(method="repeatedcv", number=10, repeats=1),
    tuneGrid = my_grid)


```



```{r}

print(xgboost.model)

```
```{r}
confusionMatrix(xgboost.model)
```
```{r}
#when comparing models train control object should be the same 

results <- resamples(list(CART=rpart.model, 
randomForest = rf.model, xgboost = xgboost.model))
summary(results)

```

```{r}
scales <- list(x=list(relation="free"), y=list(relation= "free"))
dotplot(results, scales=scales, conf.level = 0.95)

```
```{r}
carPrice <- carPrice[, c(10:14,17,19:26)]

```



```{r}
head(carPrice)
```
```{r}
#my answer 
rpart.model_NUMPRED <- train(price~.,
    data = carPrice,
    method = "rpart",
    metric = "RMSE",
    trControl = trControl2)

```

```{r}
tree.model.num <- train(price ~ ., data=carPrice,  metric="RMSE",
                    method="rpart", trControl= trControl2)
print(tree.model.num)
varImp(tree.model.num)
fancyRpartPlot(tree.model.num$finalModel)
```

```{r}
min(tree.model.num$results$RMSE)/mean(carPrice$price)
```



```{r}
min(rpart.model_NUMPRED$results$RMSE)/mean(carPrice$price) * 100
```



```{r}
#rpart.model_NUMPRED
```

```{r}

set.seed(1234)
# Run the model

rf.model.num <- train(price ~ ., data=carPrice,  
                    method="rf",
    trControl = trControl2)

print(rf.model.num)
summary(carPrice$price)
```
```{r}
min(rf.model.num$results$RMSE)/mean(carPrice$price) * 100
```

```{r}
varImp(rf.model.num)
```



## Exercise 10 - Boosted trees


```{r}
set.seed(1234)
# Run the model

xgboost.model.num <- train(price ~ ., data=carPrice,   
                    method="xgbTree",
    trControl = trControl2)


# print(xgboost.model.num)
```

```{r}
min(xgboost.model.num$results$RMSE)/mean(carPrice$price)
```


```{r}
varImp(xgboost.model.num)
```

The variables should be compared to those obtained with the other models.

## Exercise 11 - Comparing results

```{r}
results <- resamples(list(CART=tree.model.num, "random forest" = rf.model.num, xgboost = xgboost.model.num))
results
summary(results)

```

Showing the results graphically at 99% confidence.

```{r}
scales <- list(x=list(relation="free"), y=list(relation= "free"))
dotplot(results, scales=scales, conf.level = 0.95)
```
It can be seen that random forest and XGBoost had significantly less MAE and RMSE than cart (rpart). 

## Exercise 12

The student can tune the algorithms parameters.


# Creating train and test sets

```{r}
set.seed(1234)
partIndex <- createDataPartition(carPrice$price, p=0.8, list=F)
trainData <- carPrice[partIndex,]
testData <- carPrice[-partIndex,]
```


```{r}
set.seed(1234)
rpart.model.tr <- train(price ~ ., data=trainData,  
                    method="rpart",
    trControl = trControl2, na.action=na.omit)
```


# Testing

```{r}
testRes <- predict(rpart.model.tr, newdata = testData)
sqrt(mean   ((testRes - testData$price)^2        ))
```
## Exercise 13

```{r}
set.seed(1234)
rf.model.tr <- train(price ~ ., data=trainData,   
                    method="rf",
    trControl = trControl2, na.action=na.omit)
```

```{r}
rftestRes <- predict(rf.model.tr, newdata = testData)
sqrt(mean   ((rftestRes - testData$price)^2        ))
```

```{r}
set.seed(1234)
xb.model.tr <- train(price ~ ., data=trainData, 
                    method="xgbTree",
    trControl = trControl2, na.action=na.omit)
```

```{r}
xbtestRes <- predict(xb.model.tr, newdata = testData)
sqrt(mean   ((xbtestRes - testData$price)^2        ))
```

## Exercise 14


```{r}
set.seed(1234)
partIndex <- createDataPartition(titanic$Survived, p=0.8, list=F)
ttrainData <- titanic[partIndex,]
ttestData <- titanic[-partIndex,]
```


```{r}
set.seed(1234)
rpart.model.tr <- train(Survived ~ ., data=ttrainData, 
                    method="rpart",
    trControl = trControl2, na.action=na.omit)
```

```{r}
rpartRes <- predict(rpart.model.tr, newdata = ttestData)
table(rpartRes, ttestData$Survived)
```

In the table above, rows contain predictions and columns contain actual class.
The results should be commented on.


```{r}
set.seed(1234)
rf.model.tr <- train(Survived ~ ., data=ttrainData, 
                    method="rf",
    trControl = trControl2, na.action=na.omit)
```

```{r}
rfRes <- predict(rf.model.tr, newdata = ttestData)
table(rfRes, ttestData$Survived)
```

The results should be compared with those obtained with the previous model.

```{r}
set.seed(1234)
xgb.model.tr <- train(Survived ~ ., data=ttrainData, 
                    method="xgbTree",
    trControl = trControl2, na.action=na.omit)
```

```{r}
xgbRes <- predict(xgb.model.tr, newdata = ttestData)
table(xgbRes, ttestData$Survived)
```


